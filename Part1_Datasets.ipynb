{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "Part1_Datasets.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuval-ya/elections_cpp/blob/master/Part1_Datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhndKXIwsX4M"
      },
      "source": [
        "# Assignment 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-17T12:27:18.729152Z",
          "start_time": "2021-02-17T12:27:18.723022Z"
        },
        "id": "Ucy8mGOjsX4S"
      },
      "source": [
        "Submitted by:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KTEGLNmsX4T"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "As part of this assignment, we will familiarize ourselves with **PyTorch**, its basic capabilities, and use it in implementing traditional deep learning algorithms.  During this assignment, we will also practice some of the unique machine learning concepts such as cross validation, loss functions, model hyperparameters, and gradient derivation.\n",
        "We will also learn how to use **numpy** and **sklearn** packages that are most widely used in machine learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41EqUX1psX4T"
      },
      "source": [
        "## Part 1: Working with data in PyTorch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdDrf9SmsX4T"
      },
      "source": [
        "A lot of effort in solving any machine learning problem goes in to preparing the data. Several PyTorch tools are provided to simplify data loading and ultimately make your code more readable. In this part we will use pytorch *Dataset* and *DataLoader* classes which are part of PyTorch's *torch.util.data* package. Being able to use these classes properly is one of the essential skills you should have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-13T08:55:44.216669Z",
          "start_time": "2021-04-13T08:55:44.199256Z"
        },
        "id": "1xyD-zXSsX4U"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import unittest\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "plt.rcParams.update({'font.size': 12})\n",
        "torch.random.manual_seed(1904)\n",
        "test = unittest.TestCase()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz3wxBrmtSDh"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My\\ Drive/\n",
        "from utils import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j57iDqXpsX4V"
      },
      "source": [
        "### Datasets\n",
        "<a id=part1_1></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRTXpma5sX4V"
      },
      "source": [
        "The **Dataset** class is an abstraction over a sequence of python objects,\n",
        "each representing a sample (with or without a label). The main purpose of this function is to load a single (possibly labeled) sample from some source into memory and transform it into some usable representation.\n",
        "\n",
        "Lets create a demonstration Dataset that returns single color images. It should:\n",
        "- Return random tensors of size HxWx3.\n",
        "- Initialize each returned tensor with a same random number from the range [0, 255], we will treat this number as the image's label.\n",
        "- Label each returned tensor with a the number the tensor if filled.\n",
        "- Return a total of num_samples labeled images.\n",
        "\n",
        "__ getitem __ is a build-in function return the sample and its label, using index.\n",
        "\n",
        "__ len __ is a build-in function return the length of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-13T08:18:11.749373Z",
          "start_time": "2021-04-13T08:18:11.738669Z"
        },
        "id": "4BYL48d_sX4W"
      },
      "source": [
        "class SingleColorImageDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A dataset returning single color images of specified dimensions\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_samples, W, H):\n",
        "        \"\"\"\n",
        "        :param num_samples: Number of samples (labeled images in the dataset)\n",
        "        :param num_classes: Number of classes (labels)\n",
        "        :param W: Image width\n",
        "        :param H: Image height\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_samples = num_samples\n",
        "        self.image_dim = (1, H, W)\n",
        "        # TODO: Create a random labels and tensors with dimension of self.image_dim.\n",
        "        # Fill these tensors with the label and save it as internal variables.\n",
        "        x = []\n",
        "        y = []\n",
        "\n",
        "        for i in range(num_samples):\n",
        "          label = random.randint(0, 255)\n",
        "          x.append(torch.full(size=(3, H, W), fill_value=label))\n",
        "          y.append(label)\n",
        "\n",
        "        self._fetures = x\n",
        "        self._labels = y \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # TODO: Create a single color image tensor and return it.\n",
        "        sample = self._fetures[index]\n",
        "        label = self._labels[index]\n",
        "        return sample, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-13T08:19:05.970924Z",
          "start_time": "2021-04-13T08:19:05.418859Z"
        },
        "id": "qNiJM3gtsX4X"
      },
      "source": [
        "dataset = SingleColorImageDataset(1000, 32, 32)\n",
        "sample, label = dataset[0]\n",
        "assert label == sample[0][0][0]\n",
        "\n",
        "plot_dataset (dataset, 12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s7o85ACsX4Y"
      },
      "source": [
        "### Built-in Datasets and Transforms\n",
        "<a id=part1_2></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK-S3RfEsX4Y"
      },
      "source": [
        "Now, after we have created a simple Dataset to see how it works, we will load one of pytorch's built-in datasets - CIFAR-10. This is a famous dataset consisting of 60,000 small 32x32 images classified into 10 classes. You can read more about it [here](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
        "\n",
        "The torchvision package has built-in Dataset classes that can download the data to a local folder,\n",
        "load it, transform it using arbitrary transform functions and iterate over the resulting samples.\n",
        "\n",
        "Run the following code block to download and create a CIFAR-10 Dataset. It won't be downloaded again if already present."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-13T08:21:21.658506Z",
          "start_time": "2021-04-13T08:21:20.485898Z"
        },
        "id": "zA-wx23bsX4Z"
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as tvtf\n",
        "\n",
        "cfar10_classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "cifar10_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data/cifar-10/', download=True, train=True,\n",
        "    transform=tvtf.ToTensor() # Convert PIL image to pytorch Tensor\n",
        ")\n",
        "\n",
        "print('Number of samples:', len(cfar10_classes))\n",
        "# Plot them with a helper function\n",
        "plot_dataset( cifar10_dataset, 9, classes = cfar10_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5b7deud6DOm"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEn2BFedsX4Z"
      },
      "source": [
        "Now, suppose you want to train your model on the first 3 classes from cifar 10, 'plane', 'car', 'bird'. In order to achieve this you will need to wrap the cifar 10 dataset with your own dataset, that will return samples that only from these classes. You may want to use source_dataset.targets to get a list of targets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-13T08:21:22.644013Z",
          "start_time": "2021-04-13T08:21:22.634445Z"
        },
        "id": "CvYsoflvsX4Z"
      },
      "source": [
        "class SubsetDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A dataset that wraps another dataset, returning a subset from it.\n",
        "    \"\"\"\n",
        "    def __init__(self, source_dataset: Dataset, desired_classes: list):\n",
        "        \"\"\"\n",
        "        Create a SubsetDataset from another dataset.\n",
        "        :param source_dataset: The dataset to take samples from.\n",
        "        :param desired_classes: List of allowed classes from the original dataset.\n",
        "        \"\"\"\n",
        "\n",
        "        self.source_dataset = source_dataset\n",
        "        self.desired_classes = desired_classes\n",
        "        # ====== YOUR CODE: ======  \n",
        "        x = []\n",
        "        for i in range(len(source_dataset)):\n",
        "          if source_dataset.targets[i] in desired_classes:\n",
        "            x.append(source_dataset[i])\n",
        "\n",
        "        self.test_data = x\n",
        "        # ========================\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # TODO: Return dataset that contains just samples that their class is in desired_classes \n",
        "\n",
        "        # ====== YOUR CODE: ======\n",
        "        return self.test_data[index]\n",
        "        # ========================\n",
        "\n",
        "    def __len__(self):\n",
        "        # ====== YOUR CODE: ======\n",
        "        return len(self.test_data)\n",
        "        # ========================\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-13T08:21:24.351614Z",
          "start_time": "2021-04-13T08:21:23.222397Z"
        },
        "id": "SWkcnOKusX4a"
      },
      "source": [
        "subset_len = 5000\n",
        "subset_offset = 1234\n",
        "cifar10_train_subset = SubsetDataset(cifar10_dataset, [0,1,2])\n",
        "\n",
        "# Tests\n",
        "test.assertEqual(len(cifar10_train_subset), 15000)\n",
        "classes = []\n",
        "for _, _class in cifar10_train_subset:\n",
        "    classes.append(_class)  \n",
        "test.assertEqual(set(classes), set([0,1,2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBcqgiyKsX4a"
      },
      "source": [
        "### DataLoaders and Samplers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4EVLkeLsX4a"
      },
      "source": [
        "We have seen that a Dataset is simply an iterable returning samples by index.\n",
        "We would like to be able to load batches of data, in order to train our model efficiently.\n",
        "A DataLoader samples a batch of samples from the dataset according to logic defined by a Sampler object.\n",
        "The sampler decides how to divide the dataset into batches of N samples. The DataLoader also handles loading samples in parallel to speed up batch creation.\n",
        "\n",
        "Memory usage is a major motivation here. By combining a DataLoader with a Dataset we are able to easily control memory constraints by simply adjusting the batch size.  This is important since large\n",
        "datasets may not fit in memory of most machines.\n",
        "A DataSet can lazily load samples from disk, and the DataLoader can sample random samples from it in parallel, providing us with a straightforward but high-performance way of iterating over random batches from our dataset without needing to hold it all in memory.\n",
        "\n",
        "Let's create a basic DataLoader for our CIFAR-10 dataset.\n",
        "Run the follwing code block multiple times and observe that different samples are shown each time in the first few batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-13T08:22:42.314242Z",
          "start_time": "2021-04-13T08:22:39.826584Z"
        },
        "id": "QtdHVZRqsX4b"
      },
      "source": [
        "# Create a simple DataLoader that partitions the data into batches\n",
        "# of size N=8 in random order, using two background proceses\n",
        "cifar10_train_dataloader = torch.utils.data.DataLoader(\n",
        "    cifar10_dataset, batch_size=9, shuffle=True, num_workers=2\n",
        ")\n",
        "\n",
        "# Iterate over batches sampled with our DataLoader\n",
        "num_batches_to_show = 5\n",
        "for idx, (images, classes) in enumerate(cifar10_train_dataloader):\n",
        "    plot_tensors_as_images(images, 9)\n",
        "    if idx >= num_batches_to_show - 1:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BylQAGNVsX4c"
      },
      "source": [
        "### Training, Validation and Test Sets\n",
        "<a id=part1_4></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Khi02aVosX4c"
      },
      "source": [
        "Now we are ready to prepare our data for training and evaluation.So far we've learnt in class that we will use two dataset, train set and test set, but practicly we will use 3 datasets. The reason is the hyperparameters we need to tune. As you remember, there are several hyperprameters in the model, such as the learning rate or the number of layers, that we need to set. How will we know which value to choose? We will use a seperated set - validation set. You can reed about is also [here](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets). Therefore we want to divide our dataset into 3 partitions - train, validation and test. We will train our model on the training set, choose its hyperparameters using the validation set and evaluate the final model preformance on the test set.\n",
        "\n",
        "![img](https://www.brainstobytes.com/content/images/2020/01/Sets.png)\n",
        "\n",
        "Now, you will implement a funciton the split the given dataset into 2 dataloaders, train and validation, using predefined ratio.\n",
        "You may want to use the sklearn.model_selection.train_test_split function and pytorch DataLoader object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-13T12:42:53.754245Z",
          "start_time": "2021-04-13T12:42:53.736560Z"
        },
        "id": "9yYMOYgdsX4c"
      },
      "source": [
        "def create_train_validation_loaders(dataset: Dataset, \n",
        "                                         validation_ratio,\n",
        "                                         batch_size=100):\n",
        "    \"\"\"\n",
        "    Splits a dataset into a train and validation set, returning a\n",
        "    DataLoader for each.\n",
        "    :param dataset: The original dataset.\n",
        "    :param validation_ratio: Ratio (in range 0,1) of the validation set size to\n",
        "        total dataset size.\n",
        "    :param batch_size: Batch size the loaders will return from each set.\n",
        "    :return: A tuple of train, validation and test DataLoader instances.\n",
        "    \"\"\"\n",
        "    if not(0.0 < validation_ratio < 1.0):\n",
        "        raise ValueError(validation_ratio)\n",
        "        \n",
        "        \n",
        "    # TODO: Create two DataLoader instances, dataloader_train and dataloader_valid.\n",
        "    # They should together represent a train/validation split of the given\n",
        "    # dataset. Make sure that:\n",
        "    # 1. Validation set size is validation_ratio * total number of samples.\n",
        "    # 2. No sample is in both datasets. You can select samples at random\n",
        "    #    from the dataset.\n",
        "    # 3. you use shuffle=True in the train dataloader and shuffle=False in the validation dataloder\n",
        "    \n",
        "\n",
        "    # ====== YOUR CODE: ======\n",
        "    raise NotImplementedError()\n",
        "    # ========================\n",
        "\n",
        "    return dl_train, dl_valid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-13T12:42:58.563428Z",
          "start_time": "2021-04-13T12:42:54.242281Z"
        },
        "id": "ayFwDMiKsX4c"
      },
      "source": [
        "validation_ratio = 0.15\n",
        "dl_train, dl_valid = create_train_validation_loaders(cifar10_dataset,\n",
        "                                                     validation_ratio = validation_ratio)\n",
        "\n",
        "\n",
        "train_size = len(dl_train.dataset)\n",
        "valid_size = len(dl_valid.dataset)\n",
        "print('Training set size: ', train_size)\n",
        "print('Validation set size: ', valid_size)\n",
        "\n",
        "# Tests\n",
        "test.assertEqual(train_size+valid_size, len(cifar10_dataset), \"Incorrect total number of samples\")\n",
        "test.assertEqual(valid_size, validation_ratio * (train_size + valid_size), \"Incorrect validation ratio\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}